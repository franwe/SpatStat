---
title: "Assignment Block A"
author: "Bianca Neubert, Franziska Wehrmann"
output: pdf_document
header-includes:
   - \usepackage{ dsfont }
fig_width: 2 
fig_height: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gstat)
library(geoR)
library(leaflet)
library(sp)
library(lattice)
library(viridis)
library(dplyr)
library(rgdal)
library(raster)
library(INLA)
# setwd("~/Uni/Semester_10/Spatial_Statistics/Assignment_A")
source("build.convex.grid.R")
knitr::opts_chunk$set(fig.width=3, fig.height = 2.5)
```

# Theory

## Spatial Processes - Moving Average
Define a process over the set of \(n\) locations \(s_1,...,s_n \in S\). For \(\epsilon(s_1),...,\epsilon(s_n) \sim (0,\sigma^2)\), iid, we define $$Y(s_i)=\epsilon(s_i) + \beta \cdot \sum_{j \in N(s_i)} \omega_{ij} \epsilon(s_j) \ \ ,i = 1...n$$
where \(N(s_i) := \{j: \ \Vert s_i-s_j \Vert < \delta \} \), \(\omega_{ij}\) are MA weights and \(\beta\) controls for the strength of spatial interrelation.

1. Determine \(\mathbb{E}[Y(s_i)]\) for \(i \in \{1,...,n\}\):
\begin{align*}
\mathbb{E}[Y(s_i)] &= \mathbb{E} \left[\epsilon(s_i) + \beta \cdot \sum_{j \in N(s_i)} \omega_{ij} \epsilon(s_j) \right]\\
&= \mathbb{E}[\epsilon(s_i)] + \beta \cdot \sum_{j \in N(s_i)} \omega_{ij} \mathbb{E}[\epsilon(s_j)]\\
&= 0 + \beta \cdot \sum_{j \in N(s_i)} \omega_{ij} \cdot 0\\
&= 0
\end{align*}
Here, we use that \(\mathbb{E}[\epsilon(s_i)0] = 0\) for all \(i\in \{1,...,n\}\) and the linearity of the expectation.

2. Determine \(\mathbb{V}[Y(s_i)]\) for \(i \in \{1,...,n\}\):
\begin{align*}
\mathbb{V}[Y(s_i)] &= \mathbb{V}\left[ \epsilon(s_i) + \beta \cdot \sum_{j \in N(s_i)} \omega_{ij} \epsilon(s_j) \right]\\
&= \mathbb{V}[\epsilon(s_i)] + \mathbb{V}\left[ \beta \cdot \sum_{j \in N(s_i)} \omega_{ij} \epsilon(s_j) \right] + 2\mathbb{C}\text{ov}\left[\epsilon(s_i), \beta \cdot \sum_{j \in N(s_i)} \omega_{ij} \epsilon(s_j)\right]\\
&= \sigma^2 + \beta^2 \mathbb{V}\left[\sum_{j \in N(s_i)} \omega_{ij} \epsilon(s_j) \right] + 2\beta \sum_{j \in N(s_i)} \omega_{ij} \mathbb{C}\text{ov} [\epsilon(s_i), \epsilon(s_j)]\\
&= \sigma^2 + \beta^2 \sum_{j \in N(s_i)} \omega_{ij}^2  \mathbb{V}\left[ \epsilon(s_j) \right] + 2\beta \omega_{ii} \sigma^2 \\
&= \sigma^2 \left( 1 + \beta^2 \sum_{j \in N(s_i)} \omega_{ij}^2 + 2 \beta \omega_{ii} \right)
\end{align*}
Here, we use that \(s_i \in N(s_i)\), \(\mathbb{C}\text{ov}[\epsilon(s_i), \epsilon(s_j)] = 0\) for \(i \not= j\) and else \(\sigma^2\).

3. Determine \(\mathbb{C}\text{ov} [Y(s_i), Y(s_j)]\) for \(i,j \in \{1,...,n\}\) and  \(i \not= j\):
      \begin{align*}
      \mathbb{C}\text{ov}[Y(s_i), Y(s_j)] &= \mathbb{C}\text{ov} \left[ \epsilon(s_i) + \beta \cdot \sum_{l \in     N(s_i)} \omega_{il} \epsilon(s_l), \epsilon(s_j) + \beta \cdot \sum_{k \in N(s_j)} \omega_{jk} \epsilon(s_k) \right]\\
      &= \mathbb{C}\text{ov} [\epsilon(s_i), \epsilon(s_j)] + \mathbb{C}\text{ov} \left[\epsilon(s_i),  \beta \cdot \sum_{k \in N(s_j)} \omega_{jk} \epsilon(s_k)\right]  \\
      & \ \ \  + \mathbb{C}\text{ov} \left[ \beta \cdot \sum_{l \in     N(s_i)} \omega_{il} \epsilon(s_l), \epsilon(s_j)\right] + \mathbb{C}\text{ov} \left[\beta \cdot \sum_{l \in N(s_i)} \omega_{il} \epsilon(s_l) , \beta \cdot \sum_{k \in N(s_j)} \omega_{jk} \epsilon(s_k)\right]\\
      &= 0 + \beta \cdot \sum_{k \in N(s_j)} \omega_{jk} \mathbb{C}\text{ov} [\epsilon(s_i), \epsilon(s_k)] + \beta \cdot \sum_{l \in     N(s_i)} \omega_{il} \mathbb{C}\text{ov} \left[\epsilon(s_l),  \epsilon(s_j)\right]\\
      & \ \ \  + \beta^2 \sum_{l \in N(s_i)} \sum_{k \in N(s_j)}  \omega_{il}\omega_{jk}  \mathbb{C}\text{ov} [\epsilon(s_l), \epsilon(s_k)]\\
      &= \beta \sigma^2 \omega_{ji}  \mathds{1}_{\{i \in N(s_j)\}} + \beta \sigma^2 \omega_{ij} \mathds{1}_{\{j \in N(s_i)\}} + \beta^2 \sigma^2 \sum_{l = 1}^n \mathds{1}_{\{j \in N(s_i)\cap N(s_j)\}} \omega_{il}\omega_{jl}\\
      &= \mathds{1}_{\{i \in N(s_j)\}} \beta \sigma^2 (\omega_{ji}+ \omega_{ij}+ \beta \sum_{l=1}^n \mathds{1}_{\{j \in N(s_i)\cap N(s_j)\}} \omega_{ij}\omega_{jl})
       \end{align*}    
where \(\mathds{1}\) indicates the indicator function and we have that \(i \in N(s_j)\) is equivalent to \(j \in N(s_i)\).

4. Is this a second-order stationary process?
The random field \(\{Y(s)\}\) is second-order stationary if
  i) \(\mathbb{E}[Y(s)] = m\) for all \(s \in S\). This is fulfilled with \(m = 0\), see 3.
  ii) \(\mathbb{E}[Y(s)Y(s+h)] = C(h)\) for any \(s, s+h \in S\) and \(C\) only depends on \(h\). This is not the case without further constraints on the weights.
  
  Hence \(\{Y(s)\}\) is not a second-order stationary process.


## (Semi-)Variogram and Correlation Function
Let \(Y = \{Y(s), s \in S\}\) denote a second-order IRF where \(S \subset \mathbb{R}^2\) with \(m = 0\) and covariogram \(C(h)\), \(C(0) = \sigma^2 = 1\).

To show:  For the correlogram \(\rho(h)\) of \(Y\) we have that $$\gamma(h) = 1 - \rho(h)$$
Proof: First we have per definition and with \(C(0)= 1\) that \(\rho(h) = \frac{C(h)}{C(0)} = C(h) \). For the variogram \(\gamma\) we get with its defintion and the defintion of \(C\) for \(s, s+h \in S\)
\begin{align*}
2\gamma(h) &= \mathbb{V}[Y(s+h)-Y(s)]\\
  &= \mathbb{V}[Y(s+h)] + \mathbb{V}[Y(s)] - 2\mathbb{C}\text{ov}[Y(s+h), Y(s)]\\
  &= \mathbb{C}\text{ov}[Y(s+h),Y(s+h+0)] + \mathbb{C}\text{ov}[Y(s), Y(s+0))] -2 \mathbb{C}\text{ov}[Y(s+h),Y(s)]\\
  &= C(0) + C(0) - 2 C(h)\\
  &= 2 - 2 C(h).
\end{align*}
From this we get for the correlogram that
\begin{align*}
\gamma(h) = 1 - C(h) = 1 - \rho(h)
\end{align*}
which concludes the proof.


# Computation

## The Jura Data
For this task, we look at the Jura data, originally collected by the Swiss federal Institute of Technology at Lausanne which contains information on the concentrations of seven heavy metals (cadium, cobalt, chromium, copper, nickel, lead and zinc) in the top soil at each location. For this, we use the packages **gstat** and **geoR**. First, let us have a look at the data.
```{r, fig.width=7}
jura <- read.table("jura.txt", header = TRUE)
head(jura)
```
Now, we want our data to be saved as a SpatialPointsDataFrame, so we add coordinates (We keep `jura` just for convenience). Also, we generate a convex grid using the function `build.convex.grid` as a surface for predictions later and ensure that it is of class SpatialPoints:
```{r, fig.width=7, echo=T, warning=FALSE, message=FALSE, results="hide"}
cj <- read.table("jura.txt", header = TRUE)
coordinates(cj) = ~x+y
jg <- data.frame(build.convex.grid(jura[,1], jura[,2],10000))
names(jg) <- c("x","y")
jg <- SpatialPoints(jg)

```
For this task, we only consider the concentration of nickel (`Ni`).To get a first impression, we map the measured concentration of nickel with `spplot` and `bubble` (the bubble plot is not very helpful in this display, but we did not want to include very big plots).


```{r, fig.height= 3.5, fig.width= 3.5, echo = FALSE}
spplot(cj, zcol = "Ni")
bubble(cj, "Ni", col="blue", main = "Ni concentrations")
```

We could also compute distances (e.g. Euclidean distance) and look if there is a relation between it and the concentration of nickel:
```{r}
d <- dist(jura[1:2])
xyplot(log(Ni) ~ sqrt(d), as.data.frame(jura))
```

**Why does it not seem to be as if there is a relation? And what exactly was plotted here?**

### (Semi-)Variograms
Let us compute (semi-)variograms from the data using the `variogram` function, where we assume no trend for variable `log(Ni)` and fit different variogram models (Spherical, Exponential, Gaussian and Matern). First, we use the default classical method of moments variogram estimate for `variogram` and produce plots.
```{r}
v1  = variogram(log(Ni)~1, cj)
v11.fit <- fit.variogram(v1, model = vgm(1, "Sph", 10, 1))
v12.fit <- fit.variogram(v1, model = vgm(1, "Exp", 10, 1))
v13.fit <- fit.variogram(v1, model = vgm(1, "Gau", 10, 1))
v14.fit <- fit.variogram(v1, model = vgm(1, "Mat", 10, 1))
```
````{r, echo= FALSE}
plot(v1, v11.fit, main = "Spherical")
plot(v1, v12.fit, main = "Exponential")
plot(v1, v13.fit, main = "Gaussian")
plot(v1, v14.fit, main = "Matern")
plot(v1, v15.fit, cutoff = 3, "Nugget")
plot(v1, v16.fit, "Linear")
```

We can also use Cressie's version for `variogram` and, hence, compute a 
robustified version, here we only fit the spherical model.
```{r}
v2 = variogram(log(Ni)~1, cressie  = TRUE, cj)
v2.fit <- fit.variogram(v2, model = vgm(1, "Sph", 10, 1 ))
```
```{r, echo = FALSE}
plot(v2, v2.fit)
```

Finally, we can perform kriging with the `krige` function where we use the spherical fitted model from Cressie's version. With `ssplot` we can plot the predictions and its variances.

1. Ordinary Kriging (OK)

```{r, message = FALSE, warning=FALSE, results="hide"}
x <- krige(log(Ni)~1, cj, jg, model = v2.fit)
```

```{r, echo=FALSE, fig.height=3, fig.width=3}
spplot(x["var1.pred"], main = "ordinary kriging predictions")
spplot(x["var1.var"],  main = "ordinary kriging variance")
```

2. Universal Kriging (UK)

```{r, message = FALSE, warning= FALSE, results="hide"}
y <- krige(log(Ni)~x+y, cj, jg, model = v2.fit)
```

```{r, echo=FALSE, fig=3, fig.height=3}
spplot(y["var1.pred"], main = "universal kriging predictions")
spplot(y["var1.var"],  main = "universal kriging variance")
```

**Why is there not much difference?**


In addition, we can perform variography and kriging with the R package **geoR**. For this we need to transform the data into a geodata object. We again just look at the nickel concentration which is in column \(9\) of the `jura` dataframe. Inspect the data and the geodata object.
```{r}
ju.geo <- as.geodata(jura, data.col = 9)
attributes(ju.geo)
summary(ju.geo)
```
Here, there are multiple possible functions to compute variograms. First, we use `variog` to compute an empirical variogram again with the classical method of moments (``vario1`) and with the estimator suggested by Cressie (`vario2`) to get an object of class `variogram` which is needed for the `variofit` function. 
```{r, results="hide", message=FALSE}
vario1 <- variog(ju.geo)
vario2 <- variog(ju.geo, estimator.type = "modulus")
```

```{r, echo=FALSE}
plot(vario1, main="vario1")
plot(vario2, main="vario2")
```

Now, we can apply `variofit` with both models. For the first, we fit the default linear model and for the second we fit a matern variogram with cressie weights in the loss function.
```{r}
f1 <- variofit(vario1)
f2 <- variofit(vario2, cov.model = "matern", weights = "cressie")
```

```{r, echo=FALSE}
plot(vario1)
lines(f1)
plot(vario2)
lines(f2)
```

**Why do we have so steep curves? Is there an error?**

Next, we use `likfit` to estimate a variogram with maximum likelihood (ML) estimation - the default - and resticted maximum likelihood (REML) estimation. In the plot one can compare the empirical variogram `vario1` with the result from `ml`(black line) and `reml`(red line).
```{r, results="hide", message=FALSE}
ml <- likfit(ju.geo, ini.cov.pars = c(0.5, 0.5))
reml <- likfit(ju.geo, ini.cov.pars = c(0.5,0.5), lik.method = "REML")
```

```{r, fig.width=3, fig.height=3, echo=FALSE}
plot(vario1)
lines(ml)
lines(reml, col = "red")
```

Then, compute profile likelihoods for model parameters  with `proflik`.
```{r, results="hide", message=FALSE}
pl  <- proflik(ml, ju.geo,ill.values=seq(0.5, 1.5, l=4),
               range.val=seq(0.1, .5, l=4))
plot(pl, nlevels = 16)
```

Use `gstat::fit.variogram.reml`:
```{r, fig.width=3, fig.height=3}
reml.fit <- fit.variogram.reml(log(Ni)~1, cj, jg, model = vgm(1, "Sph", range=5))
plot(reml.fit, cutoff=8)
```

```{r, fig.width=3, fig.height=3}
va3 <- variogram(log(Ni)~1, cj)
plot(va3)
```

**Why do they have totally different domain and values?** Semivariance with REML are probably scaled by 10, but the domain is still much larger. With a cutoff of 2, we would not see the the variogram reaching the sill yet. 

Finally, on could perform a visual modelling using the `eyefit` funciton via
```{r, eval=FALSE}
eyefit(vario1)
```

**How should one compare the results? And how can we interpret the results from the profile likelihoods? Why is there not much difference between ML and REML?**



## The Gambia Malaria Data  
For this task, we use data on malaria prevalence in children obtained at 65 villages in The Gambia. Inspect the data. We see that this data does meet the assumptions of geostatistical data since there are multiple observations for some locations (the same coordinates, that is the  same `x` and the same `y` value).
```{r}
data(gambia)
head(gambia)
dim(unique(gambia[,c("x","y")]))
```
We see that there are 65 different locations. For our anlaysis we need to transform the data which we do using the package **dplyr**. Since we are interested in the prevalence of malaria per location (number of positively tested children divided by the total number of tested children) we add this value to our data. The transformed data is saved in a dataframe `d`. 
```{r, hide="results", message=FALSE, warning=FALSE}
gambia$count <- rep(1, 2035)
d <- gambia %>% 
      group_by(x,y) %>% 
      summarize(positive = sum(pos), total = sum(count)) %>%
      mutate(prev = positive / total) %>% ungroup
```

The data is in UTM format (Easting/Northing). We change the projection to CRS("+proj=longlat +datum=WGS84"). Finally, we add the longitude and latitude variables to `d`. Inspect the transformed object.
```{r}
spd <- SpatialPoints(d[, c("x", "y")],
                     proj4string = CRS("+proj=utm +zone=28 +datum=WGS84")
)
spdt <- spTransform(spd, CRS("+proj=longlat +datum=WGS84"))

d[,c("long", "lat")] <- coordinates(spdt)
head(d)
```


Now, we construct a map with the locations of the villages and the malaria prevalence using the **leaflet** package. However, this produces html-plots which we don't know how to display here. This is the code we used (which was already provided in the assignment):
```{r, eval=FALSE}
pal <- colorBin("viridis", bins = c(0, 0.25, 0.5, 0.75, 1))
leaflet(d) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>% 
  addCircles(lng = ~long, lat = ~lat, color = ~pal(prev)) %>% 
  addLegend("bottomright", pal = pal, values = ~prev, title = "Prevalence") %>%
  addScaleBar(position = c("bottomleft"))
```

To join covariate information on the evelation in The Gambia to our model, we use the
`getData` function from the **raster** library.
```{r, warning=FALSE}
r <- getData(name = "alt", country = "GMB", mask = T)
```
We can again compute a map of the relevant raster using the capacities of the **leaflet** via
```{r, eval=FALSE}
pal <- colorNumeric("viridis", values(r), na.color = "transparent")
leaflet(d) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>% 
  addRasterImage(r, color = pal) %>%
  addLegend("bottomright",
            pal = pal, values = values(r),
            title = "Altitude") %>%
  addScaleBar(position = c("bottomleft"))
```

We now extract the altitude values at the villages locations using the `extract` function of
the **raster** package
```{r}
d$alt <- raster::extract(r, d[, c("long", "lat")])
head(d)
```
Now, we turn to fitting the prevalence model using the INLA and SPDE approach where
we consider the following specifications of the prevalences
\begin{align*}
Y_i \vert P(s_i) \sim \text{Bin}(N_i, P(s_i))
\end{align*}
where \(P(s_i)\) is the true prevalence at location \(s_i, \ i = 1,...,n\), and \(Y_i\) is the
number of positive results out of\(N_i\) people sampled at \(s_i\) such that
\begin{align*}
logit(P(s_i)) = beta0 + beta1 * altitude + f(s_i)
\end{align*}
where \(f(s_i)\) is a spatial random effect following a zero-mean Gaussian process
with Matern covariance function.

To begin with, define a mesh using the **INLA** library and plot the mesh:
```{r, results="hide"}
coo <- cbind(d$long, d$lat)
mesh <- inla.mesh.2d(
  loc = coo, max.edge = c(0.1, 5),
  cutoff = 0.01
)
```

```{r, echo=FALSE, fig.width=4, fig.height=4}
plot(mesh)
```

Here, `max.edge` determines the largest allowed triangle edge length for the outer edges and the inner edges. Changing the value for `cutoff` changes the minimum allowed distance between points and replaces points closer than this value with one single edge. So if the value is very large there are only very few points considered  and, thus, not very many triangles and if the value is very small there are all points considered and the mesh is very finely structured not combing any similar points.

For the prediction with `inla`, first, generate an index set and projection matrix and set the stochastic partial differential equation.
```{r, results="hide"}
spde <- inla.spde2.matern(mesh = mesh, alpha = 2, constr = TRUE)
A <- inla.spde.make.A(mesh = mesh, loc = coo)
indexs <- inla.spde.make.index("s", spde$n.spde)
```
Compute the predicting location through `rasterToPoints` and inspect the new object.

```{r}
s0 <- rasterToPoints(r)
dim(s0)
```
Compute a lower number of predicting locations by aggregation from `r` specifying
`fact` \(= 5\) in the `aggregate` function and extract the coordinates from this reduced
set of prediction points and compute a prediction matrix `Ap`.
```{r, results="hide"}
ag <- aggregate(r, fact = 5)
c <- rasterToPoints(ag)
coop <- c[, c("x", "y")]

Ap <- inla.spde.make.A(mesh, loc = coop)
```

We need to set up the stack data for the estimation and prediction:
```{r, results="hide"}
stk.e <- inla.stack(tag = "est",
                    data = list(y = d$positive, numtrials = d$total),
                    A = list(1, A),
                    effects = list(data.frame(b0 = 1, cov = d$alt),  s = indexs)
)

stk.p <- inla.stack(tag = "pred",
                    data = list(y = NA, numtrials = NA),
                    A = list(1, Ap),
                    effects = list(data.frame(b0 = 1, cov = c[, 3]), s = indexs)
)

stk.full <- inla.stack(stk.e, stk.p)
```
For the model, we specify the following formula.
```{r, results="hide"}
formula <- y ~ 0 + b0 + cov + f(s, model = spde)
```

Here, `0` removes the intercept and we add a covariate term `b0` so that all coovariate terms can be captured in the projection matrix. Finally, we can call `inla`:
```{r}
res <- inla(formula, family = "binomial", Ntrials = numtrials,
            control.family = list(link = "logit"),
            data = inla.stack.data(stk.full),
            control.predictor = list(compute = TRUE, link = 1, A = inla.stack.A(stk.full)))
```


To inspect the results, we again create a map of the posteriori means from the results using a predefined index. Since the predicted values correspond to points, we use th
predicted values correspond to a set of points (which could be seen in the first plot below). We can create a raster with `rasterize`. This is depicted in the second map (which again can not be seen in this pdf).
```{r, eval=FALSE}
index <- inla.stack.index(stack = stk.full, tag = "pred")$data
prev_mean <- res$summary.fitted.values[index, "mean"]
pal <- colorNumeric("viridis", c(0, 1), na.color = "transparent")

leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircles(
    lng = coop[, 1], lat = coop[, 2],
    color = pal(prev_mean)
  ) %>%
  addLegend("bottomright",
            pal = pal, values = prev_mean,
            title = "Prev."
  ) %>%
  addScaleBar(position = c("bottomleft"))

r_prev_mean <- rasterize(
  x = coop, y = ag, field = prev_mean,
  fun = mean
)

pal <- colorNumeric("viridis", c(0, 1), na.color = "transparent")

leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addRasterImage(r_prev_mean, colors = pal, opacity = 0.5) %>%
  addLegend("bottomright",
            pal = pal,
            values = values(r_prev_mean), title = "Prev."
  ) %>%
  addScaleBar(position = c("bottomleft"))
```


 






  